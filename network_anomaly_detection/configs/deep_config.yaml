# Configuration for deep autoencoder experiment
# Experiment: deep_autoencoder

# Data configuration
data:
  dataset_path: "../NSL-KDD_Dataset"
  train_file: "KDDTrain+.txt"
  test_file: "KDDTest+.txt"
  validation_split: 0.2
  batch_size: 512  # Increased for better GPU utilization
  num_workers: 4   # Optimal for most systems
  shuffle_train: true

# Model configuration
model:
  type: "deep"
  input_dim: 41
  latent_dim: 4  # Smaller latent dimension
  activation: "leaky_relu"
  use_batch_norm: true
  dropout_rate: 0.3  # Higher dropout for regularization

# Training configuration
training:
  num_epochs: 150
  learning_rate: 0.001  # Increased for faster convergence
  optimizer: "adamw"
  scheduler: "cosine"
  loss_type: "mse"
  
  early_stopping_patience: 25
  min_delta: 1e-6
  
  gradient_clip_value: 1.0  # Increased for stability
  weight_decay: 1e-4
  
  log_interval: 25  # More frequent logging
  save_interval: 15

# GPU configuration
device:
  use_cuda: true
  cuda_device: 0
  memory_efficient: true

# Evaluation configuration
evaluation:
  threshold_methods:
    - "percentile_90"
    - "percentile_95"
    - "percentile_99"
    - "optimal_f1"
    - "optimal_recall"
  error_type: "mse"
  save_plots: true
  plot_format: "png"

# Experiment tracking
experiment:
  name: "deep_autoencoder"
  description: "Deep autoencoder with more regularization"
  save_model: true
  save_results: true
  save_plots: true
  results_dir: "../results"
  models_dir: "../models"
  plots_dir: "../plots"

seed: 42